{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e56daad4-2952-4271-af3e-3327cf6f3137",
   "metadata": {},
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3043e513-4eed-432b-af0d-3dd671a3f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a62c03-6f17-40f7-809f-3a3d63949fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be72646-2e79-4d53-898f-eb7b45e3739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(image_dir, batch_size=32, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    A generator to load and preprocess images in batches.\n",
    "    Args:\n",
    "        image_dir: Directory containing images.\n",
    "        batch_size: Number of images per batch.\n",
    "        target_size: Target size for resizing images (height, width).\n",
    "    Yields:\n",
    "        Batch of images as numpy arrays normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    image_files = os.listdir(image_dir)\n",
    "    num_images = len(image_files)\n",
    "    while True:  # Infinite loop for generator\n",
    "        np.random.shuffle(image_files)  # Shuffle images each epoch\n",
    "        for i in range(0, num_images, batch_size):\n",
    "            batch_files = image_files[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            for file in batch_files:\n",
    "                img_path = os.path.join(image_dir, file)\n",
    "                img = load_img(img_path, target_size=target_size)\n",
    "                img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "                batch_images.append(img_array)\n",
    "            yield np.array(batch_images)\n",
    "\n",
    "hazy_dir=r\"C:\\Users\\atul toppo\\Documents\\7thSem_Minor_Project\\Dataset\\OTS\\part4\"\n",
    "clear_dir=r\"C:\\Users\\atul toppo\\Documents\\7thSem_Minor_Project\\Dataset\\OTS\\clear\\clear\"\n",
    "# Foggy image generator\n",
    "foggy_generator = image_generator(hazy_dir, batch_size=32)\n",
    "\n",
    "# Clear image generator\n",
    "clear_generator = image_generator(clear_dir, batch_size=32)\n",
    "\n",
    "def hazy_generator():\n",
    "    \"\"\"\n",
    "    Function-based generator for hazy images.\n",
    "    Yields batches of hazy images.\n",
    "    \"\"\"\n",
    "    while True:  # Infinite loop for continuous generation\n",
    "        for img_file in os.listdir(hazy_dir):\n",
    "            img_path = os.path.join(hazy_dir, img_file)\n",
    "            img = tf.keras.utils.load_img(img_path, target_size=(224, 224))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "            yield img\n",
    "\n",
    "def clear_generator():\n",
    "    \"\"\"\n",
    "    Function-based generator for clear images.\n",
    "    Yields batches of clear images.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for img_file in os.listdir(clear_dir):\n",
    "            img_path = os.path.join(clear_dir, img_file)\n",
    "            img = tf.keras.utils.load_img(img_path, target_size=(224, 224))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "            yield img\n",
    "\n",
    "def combined_generator(hazy_gen_func, clear_gen_func, batch_size=32):\n",
    "    \"\"\"\n",
    "    Combines two generators to yield batches of (hazy, clear) images.\n",
    "    \"\"\"\n",
    "    hazy_gen = hazy_gen_func()\n",
    "    clear_gen = clear_gen_func()\n",
    "    while True:\n",
    "        hazy_batch = []\n",
    "        clear_batch = []\n",
    "        for _ in range(batch_size):\n",
    "            hazy_batch.append(next(hazy_gen))\n",
    "            clear_batch.append(next(clear_gen))\n",
    "        yield np.array(hazy_batch), np.array(clear_batch)\n",
    "\n",
    "# Combine the generators\n",
    "train_gen = combined_generator(hazy_generator, clear_generator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2d88119-f7f5-4e2c-ad3c-6938abd235d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "steps_per_epoch = len(os.listdir(clear_dir)) // batch_size\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6432f9-d986-4a0f-aee0-4c19b6714812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim(y_true, y_pred):\n",
    "    y_true = tf.image.resize(y_true, y_pred.shape[1:3])  # Resize target to match prediction\n",
    "    return tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    y_true = tf.image.resize(y_true, y_pred.shape[1:3])  # Resize target to match prediction\n",
    "    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e39c02-7dc4-4f0a-a8a5-78b082dee542",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='loss',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "class EvaluateBatch(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Evaluate a single batch from the dataset\n",
    "        sample_hazy, sample_clear = next(train_gen)\n",
    "        predictions = model.predict(sample_hazy)\n",
    "        psnr_value = tf.reduce_mean(tf.image.psnr(sample_clear, predictions, max_val=1.0))\n",
    "        ssim_value = tf.reduce_mean(tf.image.ssim(sample_clear, predictions, max_val=1.0))\n",
    "        print(f\"Epoch {epoch+1}: Sample PSNR: {psnr_value.numpy():.2f} dB, Sample SSIM: {ssim_value.numpy():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b38efb-6204-4d6d-bc48-00c0edf16cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DehazeNet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 224, 224, 16)      1216      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 224, 224, 16)     64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 224, 224, 16)      6416      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 224, 224, 16)     64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 224, 224, 3)       1203      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,899\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers,Model\n",
    "def build_dehazenet(input_shape=(224, 224, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Feature extraction\n",
    "    x = layers.Conv2D(16, (5, 5), padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Non-linear mapping\n",
    "    x = layers.Conv2D(16, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Fusion layer\n",
    "    x = layers.Conv2D(3, (5, 5), padding='same', activation='sigmoid')(x)  # Output has 3 channels (RGB)\n",
    "\n",
    "    # Remove MaxPooling and match input size\n",
    "    outputs = x\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"DehazeNet\")\n",
    "    return model\n",
    "\n",
    "model = build_dehazenet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56eee59d-a752-40f8-9103-d1bb5c8477e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='mse',\n",
    "    metrics=['mae', psnr, ssim]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76c75ef3-7d3c-4270-b69b-b96a06ffa9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.0957 - mae: 0.2517 - psnr: 10.3779 - ssim: 0.1720\n",
      "Epoch 1: loss improved from inf to 0.09570, saving model to best_model.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "generator already executing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can adjust this\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEvaluateBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\test-env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36mEvaluateBatch.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Evaluate a single batch from the dataset\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     sample_hazy, sample_clear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(sample_hazy)\n\u001b[0;32m     28\u001b[0m     psnr_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mpsnr(sample_clear, predictions, max_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: generator already executing"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,  # You can adjust this\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=1,\n",
    "    callbacks=[lr_scheduler, early_stopping, model_checkpoint, EvaluateBatch()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217471bc-2848-43db-858f-d7527061bca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee69580-973a-43f2-979b-1556f6d8af99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
